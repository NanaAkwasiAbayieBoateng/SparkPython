{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Tutorial on Spark in Python \n",
    "You might already know Apache Spark as a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. Itâ€™s well-known for its speed, ease of use, generality and the ability to run virtually everywhere. And even though Spark is one of the most asked tools for data engineers, also data scientists can benefit from Spark when doing exploratory data analysis, feature extraction, supervised learning and model evaluation.\n",
    "\n",
    "#### Welcome to Apache Spark with Python\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "[http://spark.apache.org](http://spark.apache.org)\n",
    "\n",
    "In this notebook, we'll train two classifiers to predict survivors in the Titanic dataset. We'll use this classic machine learning problem as a brief introduction to using Apache Spark local mode in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark  \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a **SparkContext**, the main object in the Spark API. This call may take a few seconds to return as it fires up a JVM under the covers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample the data\n",
    "We point the context at a CSV file on disk. The result is a RDD, not the content of the file. This is a  <span style=\"color:red\">Spark transformation</span> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_rdd = sc.textFile(\"/Users/nanaakwasiabayieboateng/Documents/memphisclassesbooks/DataMiningscience/Capitalone/green_tripdata_2016-09.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We query RDD for the number of lines in the file. The call here causes the file to be read and the result computed. This is a Spark action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We query for the first five rows of the RDD. Even though the data is small, we shouldn't get into the habit of pulling the entire dataset into the notebook. Many datasets that we might want to work with using Spark will be much too large to fit in memory of a single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,RatecodeID,PULocationID,DOLocationID,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type',\n",
       " '',\n",
       " '2,2016-09-01 00:58:21,2016-09-01 01:11:46,N,1,92,82,1,3.34,12.5,0.5,0.5,1,0,,0.3,14.8,1,1,,',\n",
       " '2,2016-09-01 00:49:50,2016-09-01 01:05:57,N,1,83,92,2,3.78,14.5,0.5,0.5,0,0,,0.3,15.8,2,1,,',\n",
       " '2,2016-09-01 00:06:58,2016-09-01 00:15:13,N,1,93,223,1,4.84,15,0.5,0.5,0,0,,0.3,16.3,2,1,,']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a header row followed by a set of data rows. We filter out the header to define a new RDD containing only the data rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get the heading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VendorID,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,RatecodeID,PULocationID,DOLocationID,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = raw_rdd.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = raw_rdd.filter(lambda line: line != header)\n",
    "data_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a random sample of the data rows to better understand the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,2016-09-19 06:39:52,2016-09-19 06:53:11,N,1,166,247,1,4.83,16,0,0.5,0,0,,0.3,16.8,2,1,,',\n",
       " '2,2016-09-01 08:36:59,2016-09-01 09:02:49,N,1,42,230,1,4.84,19.5,0,0.5,0,0,,0.3,20.3,1,1,,',\n",
       " '1,2016-09-27 22:28:00,2016-09-27 22:40:15,N,1,196,28,1,3.00,12,0.5,0.5,2.65,0,,0.3,15.95,1,1,,',\n",
       " '1,2016-09-23 20:55:58,2016-09-23 20:56:07,N,1,145,145,1,.00,2.5,0.5,0.5,0,0,,0.3,3.8,3,1,,',\n",
       " '2,2016-09-11 00:54:55,2016-09-11 01:28:47,N,1,247,85,1,16.76,47,0.5,0.5,0,5.54,,0.3,55.79,1,1,,']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd.takeSample(False, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labeled points (i.e., feature vectors and ground truth)\n",
    "Now we define a function to turn the passenger attributions into structured LabeledPoint objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def row_to_labeled_point(line):\n",
    "    '''\n",
    "    Builds a LabelPoint consisting of:\n",
    "    \n",
    "    survival (truth): 0=no, 1=yes\n",
    "    ticket class: 0=1st class, 1=2nd class, 2=3rd class\n",
    "    age group: 0=child, 1=adults\n",
    "    gender: 0=man, 1=woman\n",
    "    '''\n",
    "    passenger_id, klass, age, sex, survived = [segs.strip('\"') for segs in line.split(',')]\n",
    "    klass = int(klass[0]) - 1\n",
    "    \n",
    "    if (age not in ['adults', 'child'] or \n",
    "        sex not in ['man', 'women'] or\n",
    "        survived not in ['yes', 'no']):\n",
    "        raise RuntimeError('unknown value')\n",
    "    \n",
    "    features = [\n",
    "        klass,\n",
    "        (1 if age == 'adults' else 0),\n",
    "        (1 if sex == 'women' else 0)\n",
    "    ]\n",
    "    return LabeledPoint(1 if survived == 'yes' else 0, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split for training and test\n",
    "We split the transformed data into a training (70%) and test set (30%), and print the total number of items in each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
